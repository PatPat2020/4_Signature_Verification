{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder location/creation and functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forlder dataset exists\n"
     ]
    }
   ],
   "source": [
    "# quick check and folder creation if it was removed\n",
    "if not os.path.isdir('../dataset'):\n",
    "    os.mkdir('../dataset')\n",
    "else:\n",
    "    print('forlder dataset exists')\n",
    "    \n",
    "# folder locations\n",
    "dataset_fold = os.path.join('..', 'dataset')\n",
    "\n",
    "groundtruth_fold = os.path.join('..', 'groundtruth')\n",
    "enrollment_fold = os.path.join(groundtruth_fold, 'enrollment')\n",
    "verification_fold = os.path.join(groundtruth_fold, 'verification')\n",
    "\n",
    "        \n",
    "def dictionary_creation(file):\n",
    "    dictionary = {}    \n",
    "    \n",
    "    with open(file) as users:  \n",
    "#         Data is a big string containing all users' ID\n",
    "        data = users.read()\n",
    "#         We split this string to have each number individually into substrings\n",
    "        l_data = data.split( )\n",
    "        \n",
    "        for user in l_data:\n",
    "#             The keys of the disctionary are the users' ID, and the value is an empty list for now that will contain arrays of signature data\n",
    "            dictionary[user] = list()\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open('./../dataset/' + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        \n",
    "\n",
    "def normalize_vect(variable):\n",
    "    r_variable = variable.reshape(-1, 1)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_variable = scaler.fit_transform(r_variable)\n",
    "    \n",
    "#     normalized_variable = normalized_variable.reshape(np.shape(variable))\n",
    "    \n",
    "    return normalized_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation/Loading of the enrollment dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the enrollment dictionary\n",
      "Enrollment dictionary complete\n"
     ]
    }
   ],
   "source": [
    "# This dictionary contains 30 elements refered to with keys (IDs of the users).\n",
    "# For each element, the value is a list containing 5 arrays containing the data of signatures (here 5 genuine signatures of the users).\n",
    "\n",
    "user_file = os.path.join(groundtruth_fold, 'users.txt')\n",
    "# Creation of the dictionary\n",
    "enrollment_dict = dictionary_creation(user_file)\n",
    "\n",
    "enrollment_file = os.path.join(dataset_fold, 'enrollment.pkl')\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.isfile(enrollment_file):\n",
    "    print(\"Creating the enrollment dictionary\")\n",
    "#     for each file containing data of a genuine signature\n",
    "    for file_name in os.listdir(enrollment_fold):\n",
    "\n",
    "        file = enrollment_fold + '/' + file_name\n",
    "\n",
    "        with open(file) as f:  \n",
    "\n",
    "#            Here data is a string containing all information on seven columns:\n",
    "#             t, x, y, pressure, penup, azimuth, inclination\n",
    "            data = f.read()\n",
    "\n",
    "#            We create a list containing substrings, all float values separated --> l_data for list\n",
    "            l_data = data.split( )\n",
    "\n",
    "#            We create an array to store the data in a float format --> a_data for array\n",
    "            a_data = np.array(l_data, dtype = float)\n",
    "\n",
    "#            As we know that for each file there is seven columns, we can reshape the array that we created above --> r_data for reshaped\n",
    "            r_data = a_data.reshape(int(np.shape(a_data)[0]/7), 7)\n",
    "\n",
    "#             We put the array in the dictionary at the corresponding key\n",
    "            enrollment_dict[file_name[:3]].append(r_data)\n",
    "\n",
    "\n",
    "#     Save dictionary in a pickle file that can be load later\n",
    "    save_obj(enrollment_dict, \"enrollment\")\n",
    "    \n",
    "else:\n",
    "    #If the files exists, they are just loaded.\n",
    "    print(\"Loading the enrollment dictionary\")    \n",
    "    with open(enrollment_file, 'rb') as f:\n",
    "        dict_bin = pickle.load(f)    \n",
    "    \n",
    "    enrollment_dict.update(dict_bin)\n",
    "    \n",
    "print(\"Enrollment dictionary complete\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation/Loading of the verification dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the verification dictionary\n",
      "Verification dictionary complete\n"
     ]
    }
   ],
   "source": [
    "# This dictionary also contains 30 elements refered to with keys (IDs of the users).\n",
    "# For each element, the value is a list containing 45 arrays containing the data of signatures (here 20 genuine signatures of the users and 25 forgeries).\n",
    "\n",
    "# Creation of the dictionary\n",
    "verification_dict = dictionary_creation(user_file)\n",
    "\n",
    "verification_file = os.path.join(dataset_fold, 'verification.pkl')\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.isfile(verification_file):\n",
    "    print(\"Creating the verification dictionary\")\n",
    "#     for each file containing data of a signature, genuine or forged\n",
    "    for file_name in os.listdir(verification_fold):\n",
    "\n",
    "        file = verification_fold + '/' + file_name\n",
    "\n",
    "        with open(file) as f:  \n",
    "\n",
    "#            Here data is a string containing all information on seven columns:\n",
    "#             t, x, y, pressure, penup, azimuth, inclination\n",
    "            data = f.read()\n",
    "\n",
    "#            We create a list containing substrings, all float values separated --> l_data for list\n",
    "            l_data = data.split( )\n",
    "\n",
    "#            We create an array to store the data in a float format --> a_data for array\n",
    "            a_data = np.array(l_data, dtype = float)\n",
    "\n",
    "#            As we know that for each file there is seven columns, we can reshape the array that we created above --> r_data for reshaped\n",
    "            r_data = a_data.reshape(int(np.shape(a_data)[0]/7), 7)\n",
    "    \n",
    "            l_id_data = list()\n",
    "#             This contains the id of the signature. Usefull for future computation\n",
    "            l_id_data.append(file_name[4:6])\n",
    "#             This contians the data of the corresponding signature\n",
    "            l_id_data.append(r_data)\n",
    "        \n",
    "#             We put the list containing the id of the signature as a string and the array of the data in the dictionary at the corresponding key    \n",
    "            verification_dict[file_name[:3]].append(l_id_data)\n",
    "\n",
    "\n",
    "#     Save dictionary in a pickle file that can be load later\n",
    "    save_obj(verification_dict, \"verification\")\n",
    "    \n",
    "else:\n",
    "    #If the files exists, they are just loaded.\n",
    "    print(\"Loading the verification dictionary\")\n",
    "    with open(verification_file, 'rb') as f:\n",
    "        dict_bin = pickle.load(f)    \n",
    "    \n",
    "    verification_dict.update(dict_bin)    \n",
    "    \n",
    "print(\"Verification dictionary complete\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features creation and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the normalized enrollment dictionary\n",
      "Normalized enrollment dictionary complete\n"
     ]
    }
   ],
   "source": [
    "norm_enrollment_dict = dictionary_creation(user_file)\n",
    "\n",
    "norm_enrollment_file = os.path.join(dataset_fold, 'norm_enrollment.pkl')\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.isfile(norm_enrollment_file):\n",
    "    print(\"Creating the normalized enrollment dictionary\")\n",
    "\n",
    "    for user in enrollment_dict:\n",
    "        data = enrollment_dict[user]\n",
    "\n",
    "        for sign in data:\n",
    "            time = sign[:,0]\n",
    "            x = sign[:,1]\n",
    "            y = sign[:,2]\n",
    "            pressure = sign[:,3]\n",
    "\n",
    "    # After troubles understanding the data, we concluded that we don't need the penup feature.\n",
    "    # The pen position (x, y) is recorded even though the pen is in an up position, \n",
    "    # and the information of position and velocity will still be used to compare signatures\n",
    "\n",
    "#            Creation of the velocity features\n",
    "            n = len(time)\n",
    "            vx = np.zeros(n)\n",
    "            vy = np.zeros(n)\n",
    "\n",
    "#            The instantaneous velocity is delta_x/delta_t. \n",
    "#            So here for every signature delta_t is 0.01s and delta x is the difference between the x at time t and the x at time t+1.\n",
    "            for i in range(n):\n",
    "                if(i == 0):\n",
    "                    vx[i] = 0\n",
    "                    vy[i] = 0\n",
    "                else:\n",
    "                    vx[i] = abs(x[i]-x[i-1])/0.01\n",
    "                    vy[i] = abs(y[i]-y[i-1])/0.01\n",
    "\n",
    "#             Then we normalize each features independent\n",
    "            norm_x = normalize_vect(x)\n",
    "            norm_y = normalize_vect(y)\n",
    "            norm_vx = normalize_vect(vx)\n",
    "            norm_vy = normalize_vect(vy)\n",
    "            norm_pressure = normalize_vect(pressure)\n",
    "\n",
    "#             And we store the data into the data that we store in the dictionary\n",
    "            norm_sign = np.append(norm_x, norm_y, axis = 1)\n",
    "            norm_sign = np.append(norm_sign, norm_vx, axis = 1)\n",
    "            norm_sign = np.append(norm_sign, norm_vy, axis = 1)\n",
    "            norm_sign = np.append(norm_sign, norm_pressure, axis = 1)\n",
    "\n",
    "            norm_enrollment_dict[user].append(norm_sign)\n",
    "\n",
    "#     Save dictionary in a pickle file that can be load later\n",
    "    save_obj(norm_enrollment_dict, \"norm_enrollment\")\n",
    "    \n",
    "else:\n",
    "    #If the files exists, they are just loaded.\n",
    "    print(\"Loading the normalized enrollment dictionary\")\n",
    "    with open(norm_enrollment_file, 'rb') as f:\n",
    "        dict_bin = pickle.load(f)    \n",
    "    \n",
    "    norm_enrollment_dict.update(dict_bin)    \n",
    "    \n",
    "print(\"Normalized enrollment dictionary complete\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the normalized veification dictionary\n",
      "Normalized verification dictionary complete\n"
     ]
    }
   ],
   "source": [
    "norm_verification_dict = dictionary_creation(user_file)\n",
    "\n",
    "norm_verification_file = os.path.join(dataset_fold, 'norm_verification.pkl')\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.isfile(norm_verification_file):\n",
    "    print(\"Creating the normalized verification dictionary\")\n",
    "\n",
    "    for user in verification_dict:\n",
    "        data = verification_dict[user]\n",
    "\n",
    "        for sign in data:\n",
    "#             We only take the data and not the identification number\n",
    "            sign_d = sign[1]\n",
    "    \n",
    "            time = sign_d[:,0]\n",
    "            x = sign_d[:,1]\n",
    "            y = sign_d[:,2]\n",
    "            pressure = sign_d[:,3]\n",
    "\n",
    "#            Creation of the velocity features\n",
    "            n = len(time)\n",
    "            vx = np.zeros(n)\n",
    "            vy = np.zeros(n)\n",
    "\n",
    "#            The instantaneous velocity is delta_x/delta_t. \n",
    "#            So here for every signature delta_t is 0.01s and delta x is the difference between the x at time t and the x at time t+1.\n",
    "            for i in range(n):\n",
    "                if(i == 0):\n",
    "                    vx[i] = 0\n",
    "                    vy[i] = 0\n",
    "                else:\n",
    "                    vx[i] = abs(x[i]-x[i-1])/0.01\n",
    "                    vy[i] = abs(y[i]-y[i-1])/0.01\n",
    "\n",
    "#             Then we normalize each features independent\n",
    "            norm_x = normalize_vect(x)\n",
    "            norm_y = normalize_vect(y)\n",
    "            norm_vx = normalize_vect(vx)\n",
    "            norm_vy = normalize_vect(vy)\n",
    "            norm_pressure = normalize_vect(pressure)\n",
    "\n",
    "#             And we store the data into the data that we store in the dictionary\n",
    "            norm_sign = np.append(norm_x, norm_y, axis = 1)\n",
    "            norm_sign = np.append(norm_sign, norm_vx, axis = 1)\n",
    "            norm_sign = np.append(norm_sign, norm_vy, axis = 1)\n",
    "            norm_sign = np.append(norm_sign, norm_pressure, axis = 1)\n",
    "            \n",
    "            l_id_data = list()\n",
    "#             This contains the id of the signature. Usefull for future computation\n",
    "            l_id_data.append(sign[0])\n",
    "#             This contians the data of the corresponding signature\n",
    "            l_id_data.append(norm_sign)\n",
    "\n",
    "            norm_verification_dict[user].append(l_id_data)\n",
    "\n",
    "#     Save dictionary in a pickle file that can be load later\n",
    "    save_obj(norm_verification_dict, \"norm_verification\")\n",
    "    \n",
    "else:\n",
    "    #If the files exists, they are just loaded.\n",
    "    print(\"Loading the normalized veification dictionary\")\n",
    "    with open(norm_verification_file, 'rb') as f:\n",
    "        dict_bin = pickle.load(f)    \n",
    "    \n",
    "    norm_verification_dict.update(dict_bin)    \n",
    "    \n",
    "print(\"Normalized verification dictionary complete\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To access data:\n",
    "#### For the enrollment dictionary (and the normalized one -> norm_enrollment_dict):\n",
    "\n",
    "- enrollment_dict['001'] : gives all genuine signatures for the user \"001\"<br>\n",
    "- enrollment_dict['001'][ i ] : gives the ith genuine signature for the user \"001\"<br>\n",
    "- enrollment_dict['001'][ i ][ :, j ] : gives the jth column of the ith genuine signature for the user \"001\"<br>\n",
    "\n",
    "#### For the verification dictionary (and the normalized one -> norm_verification_dict):\n",
    "\n",
    "- verification_dict['001'] : gives all signatures (ID and data) for the user \"001\", genuine or forged<br>\n",
    "- verification_dict['001'][ i ] : gives the ith signature (ID and data) for the user \"001\"<br>\n",
    "- verification_dict['001'][ i ][ 0 ] : gives the ID number of the ith signature for the user \"001\"<br>\n",
    "- verification_dict['001'][ i ][ 1 ] : gives the data of the ith signature for the user \"001\"<br>\n",
    "- verification_dict['001'][ i ][ 1 ][ :, j ] : gives the jth column of the data of the ith signature for the user \"001\"<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
