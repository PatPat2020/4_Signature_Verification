{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder location/creation and functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forlder dataset exists\n"
     ]
    }
   ],
   "source": [
    "# quick check and folder creation if it was removed\n",
    "if not os.path.isdir('../dataset'):\n",
    "    os.mkdir('../dataset')\n",
    "else:\n",
    "    print('forlder dataset exists')\n",
    "    \n",
    "# folder locations\n",
    "dataset_fold = os.path.join('..', 'dataset')\n",
    "\n",
    "groundtruth_fold = os.path.join('..', 'groundtruth')\n",
    "enrollment_fold = os.path.join(groundtruth_fold, 'enrollment')\n",
    "verification_fold = os.path.join(groundtruth_fold, 'verification')\n",
    "\n",
    "        \n",
    "def dictionary_creation(file):\n",
    "    dictionary = {}    \n",
    "    \n",
    "    with open(file) as users:  \n",
    "#         Data is a big string containing all users' ID\n",
    "        data = users.read()\n",
    "#         We split this string to have each number individually into substrings\n",
    "        l_data = data.split( )\n",
    "        \n",
    "        for user in l_data:\n",
    "#             The keys of the disctionary are the users' ID, and the value is an empty list for now that will contain arrays of signature data\n",
    "            dictionary[user] = list()\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open('./../dataset/' + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation/Loading of the enrollment dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enrollment dictionary complete\n"
     ]
    }
   ],
   "source": [
    "# This dictionary contains 30 elements refered to with keys (IDs of the users).\n",
    "# For each element, the value is a list containing 5 arrays containing the data of signatures (here 5 genuine signatures of the users).\n",
    "\n",
    "user_file = os.path.join(groundtruth_fold, 'users.txt')\n",
    "# Creation of the dictionary\n",
    "enrollment_dict = dictionary_creation(user_file)\n",
    "\n",
    "enrollment_file = os.path.join(dataset_fold, 'enrollment.pkl')\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.isfile(enrollment_file):\n",
    "#     for each file containing data of a genuine signature\n",
    "    for file_name in os.listdir(enrollment_fold):\n",
    "\n",
    "        file = enrollment_fold + '/' + file_name\n",
    "\n",
    "        with open(file) as f:  \n",
    "\n",
    "#            Here data is a string containing all information on seven columns:\n",
    "#             t, x, y, pressure, penup, azimuth, inclination\n",
    "            data = f.read()\n",
    "\n",
    "#            We create a list containing substrings, all float values separated --> l_data for list\n",
    "            l_data = data.split( )\n",
    "\n",
    "#            We create an array to store the data in a float format --> a_data for array\n",
    "            a_data = np.array(l_data, dtype = float)\n",
    "\n",
    "#            As we know that for each file there is seven columns, we can reshape the array that we created above --> r_data for reshaped\n",
    "            r_data = a_data.reshape(int(np.shape(a_data)[0]/7), 7)\n",
    "\n",
    "#             We put the array in the dictionary at the corresponding key\n",
    "            enrollment_dict[file_name[:3]].append(r_data)\n",
    "\n",
    "\n",
    "#     Save dictionary in a pickle file that can be load later\n",
    "    save_obj(enrollment_dict, \"enrollment\")\n",
    "    \n",
    "else:\n",
    "    #If the files exists, they are just loaded.\n",
    "    print(\"Loading the enrollment dictionary\")    \n",
    "    with open(enrollment_file, 'rb') as f:\n",
    "        dict_bin = pickle.load(f)    \n",
    "    \n",
    "    enrollment_dict.update(dict_bin)\n",
    "    \n",
    "print(\"Enrollment dictionary complete\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation/Loading of the verification dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification dictionary complete\n"
     ]
    }
   ],
   "source": [
    "# This dictionary also contains 30 elements refered to with keys (IDs of the users).\n",
    "# For each element, the value is a list containing 45 arrays containing the data of signatures (here 20 genuine signatures of the users and 25 forgeries).\n",
    "\n",
    "# Creation of the dictionary\n",
    "verification_dict = dictionary_creation(user_file)\n",
    "\n",
    "verification_file = os.path.join(dataset_fold, 'verification.pkl')\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.isfile(verification_file):\n",
    "#     for each file containing data of a signature, genuine or forged\n",
    "    for file_name in os.listdir(verification_fold):\n",
    "\n",
    "        file = verification_fold + '/' + file_name\n",
    "\n",
    "        with open(file) as f:  \n",
    "\n",
    "#            Here data is a string containing all information on seven columns:\n",
    "#             t, x, y, pressure, penup, azimuth, inclination\n",
    "            data = f.read()\n",
    "\n",
    "#            We create a list containing substrings, all float values separated --> l_data for list\n",
    "            l_data = data.split( )\n",
    "\n",
    "#            We create an array to store the data in a float format --> a_data for array\n",
    "            a_data = np.array(l_data, dtype = float)\n",
    "\n",
    "#            As we know that for each file there is seven columns, we can reshape the array that we created above --> r_data for reshaped\n",
    "            r_data = a_data.reshape(int(np.shape(a_data)[0]/7), 7)\n",
    "    \n",
    "            l_id = list()\n",
    "#             This contains the id of the signature. Usefull for future computation\n",
    "            l_id.append(file_name[4:6])\n",
    "#             This contians the data of the corresponding signature\n",
    "            l_id.append(r_data)\n",
    "        \n",
    "#             We put the list containing the id of the signature as a string and the array of the data in the dictionary at the corresponding key    \n",
    "            verification_dict[file_name[:3]].append(l_id)\n",
    "\n",
    "\n",
    "#     Save dictionary in a pickle file that can be load later\n",
    "    save_obj(verification_dict, \"verification\")\n",
    "    \n",
    "else:\n",
    "    #If the files exists, they are just loaded.\n",
    "    print(\"Loading the verification dictionary\")\n",
    "    with open(verification_file, 'rb') as f:\n",
    "        dict_bin = pickle.load(f)    \n",
    "    \n",
    "    verification_dict.update(dict_bin)    \n",
    "    \n",
    "print(\"Verification dictionary complete\")      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
